---
title: "Logistic Regression & Naive Bayes"
author: "Sovanna Ramirez & Sanjana Jadhav"
date: "2023-02-17"
output:
  pdf_document: default
  html_notebook: default
  html_document:
    df_print: paged
---

In this notebook, we will be showing some models for Classification: Logistic Regression Model and Naive Bayes. Logistic Regression models relationships between one response variable and predictor variables. Naive Bayes assumes all predictors are independent and determines the conditional probability of each category of a predictor. Naive Bayes has higher bias and lower variance than Logistic Regression and you will see what this means as you read through this notebook.


## Data Exploration
This example looks at the data set **UCI Adult Income** as an intro into Logistic Regression & Naive Bayes. The data set was downloaded from here: https://www.kaggle.com/datasets/wenruliu/adult-income-dataset

The "read.csv" function takes a file path as input and loads the contents of the file into a data frame named "df."
```{r}
df <- read.csv("~/Downloads/adult.csv")
```


**Data Cleaning**
Here, we are using "sapply()" to apply a function to the entire data frame, "df." 
* The anonymous function "function(x)" uses the "sum()" and is.na()" functions to find the amount of missing values in a column. 
* A vector containing the missing values for all the columns in "df" is displayed below.
```{r}
sapply(df, function(x) sum(is.na(x)==TRUE))
```
Thankfully, the data does not have any missing values.


### str() Function
The "str()" function displays the structure of the data frame. This helps us find the data types of each of the columns.
```{r}
str(df)
```


### factor() Function
The "as.factor()" function is used to convert a column's data type to a factor variable. This way, it is easier to represent categories.
For this example, here are the variables that would be have their individual categories:
* marital.status
* income (<= 50k or > 50k)
* race
* gender
* occupation

### -c() Function
The following columns will be deleted as the data frame contains overlap/irrelevant information that may affect the accuracy.
* workclass
* fnlwgt
* education
* relationship
* capital.gain
* capital.loss
* native.country

We also use str() to view the current data frame.
```{r}
df$marital.status <- as.factor(df$marital.status)
df$income <- as.factor(df$income)
df$race <- as.factor(df$race)
df$gender <- as.factor(df$gender)
df$occupation <- as.factor(df$occupation)
df <- df[-c(2:4,8,11,12,14)]
str(df)
```


## Divide into Train/Test (80/20)
* set.seed(1234): ensures that the train/test data is the same each time the code is run
* 80% of the data is used to train the model and 20% of the test the model
* replace=FALSE ensures that there is no overlap of the data in train/test
```{r}
set.seed(1234)
i <- sample(1:nrow(df), nrow(df)*0.8,replace=FALSE)
train <- df[i,]
test <- df[-i,]
```


### summary() Function 
* The "summary()" is used to display statistics about the train data frame, consisting of values such as max/min for numerical data types and the number of occurrences of each category for factor variables.
* the "str()" is called again to visualize the structure of the train data.
```{r}
summary(train)
```


### table() Function
The "table()" function is used to view the different categories that occur in a vector and their frequencies. 
An example is shown below for the occupation column.
```{r}
table(train$occupation)
```


### head() and tail() Functions
* head(): shows the first few rows the train data frame
* tail(): shows the last few rows the train data frame
```{r}
head(train)
tail(train)
```


### cor() Function
The "cor()" function computes the correlation between two variables in a data frame.
For example, the code below calculates the correlation between "hours.per.week" and "age."
As we can say, the correlation is very close to 0. There is no correlation.
```{r}
cor(train$hours.per.week, train$age)
```


## Data Visualization
Data Visualization helps us find patterns in the data.

### hist() Function
For instance, this is a **histogram** that shows the frequency of the different ages in the train data.
```{r}
hist(train$age, col="deeppink2", main="Age Frequencies in Adult Income Data", xlab="Age")
```


### cdplot() Function
The "cdplot()" function displays the conditional density, which shows us how a numerical value affects categorical data.
For instance, the code below shows us how Education Num affects Income.
```{r}
cdplot(train$educational.num, train$income, col=c("darkolivegreen1","darkgreen"), xlab="Educational Num", ylab="Income", main="CD Plot")
```


## Logistic Regression Model
In the code below, we are creating a logistic regression model using the train data. 
* glm(): generalized linear function used for logistic regression
* income~.: all the other variables in the train data frame and predictors used to predict "income"
* data=train: we are using the train data frame
* family="binomial": a binomial logistic regression model is used as the income variable only has 2 levels (<= 50k or > 50k)
```{r}
glm1 <- glm(income~., data=train, family="binomial")
summary(glm1)
```
The purpose of this logistic regression model is to predict the probability an adult makes an income of over 50K, given other predictors such as age, education level, martial status, occupation, race, gender and hours of work per week.
The summary shows the following values:
* regression coefficient: This shows the coefficient in log odds for each of the predictors in the train data frame.
      * in our data, "Being Married," and working in "Prof-speciality" or "Tech-support" 
      has a higher probability of making an income about 50k.
* standard error: the average space between the observations and the regression line
* z-value: regression coefficient/standard error (tells us how many far  we are away from the mean and it can be positive or negative)
* p-value: indicates significance and if the value is less than 0.05, the predictor strongly influences the model
      * in our data, being a white male strongly influences the model


### Predict using the Test Data
* accuracy: number of correct predictions divided by all predictions
* confusion matrix: 
            TN (correct <=50k)       FP (incorrect > 50k)
            FN (incorrect <=50k)     TP (correct >50k)
            
            * since most of the values are on the diagonal, they are equal to their true 
            values. therefore, this model is a good fit
```{r}
probs <- predict(glm1, newdata=test, type="response")
pred <- ifelse(probs>0.5, ">50K", "<=50K")
acc <- mean(pred==test$income)
print(paste("accuracy = ", acc))
table(pred, test$income)
```


### Find Sensitivity and Specificity 
* sensitivity: the model correctly predicts 92.18% of positive case
* specificity: the model correctly only predicts 53.46% of negative case
* kappa: 0.4943 --> accounts for correct prediction by chance
```{r}
library(caret)
confusionMatrix(as.factor(pred), reference=test$income)
```


### ROC and AUC
```{r}
library(ROCR)
pr <- prediction(probs, test$income)
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf)
```
```{r}
auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]
auc
```
* ROC: plots the sensitivity against the specificity
* AUC: 0.8810923 --> this is the area under the curve and a value close to 1 is better.


## Naive Bayes . 
The A-priori probabilities are also displayed for income: <= 50k: 0.762981 and >50k: 0.2370179. These are baseline probabilities.
This model also displays the independent conditional probability for each predictor, an each predictor is independent of another.
```{r}
library(e1071)
nb1 <- naiveBayes(income~., data=train)
nb1
```

### Predict using the Test Data
* confusion matrix: 
            TN (correct <=50k)       FP(incorrect > 50k)
            FN (incorrect <=50k)     TP (correct >50k)
            
            * since most of the values are on the diagonal, they are equal to their true 
            values. therefore, this model is a good fit
* mean: number of correct predictions divided by all predictions
```{r}
pred2 <- predict(nb1, newdata = test, type="class")
table(pred2, test$income)
```
```{r}
mean(pred2 == test$income)
```
Both models only have a very slight difference in their accuracies.
* Logistic Regression: 0.82567304739482
* Naive Bayes: 0.8241376


## Strengths and Weaknesses of Logistic Regression and Naive Bayes
Both methods can handle numeric and categorical data. Logistic Regression does better on larger data whereas Naive Bayes does better on smaller data. Naive Bayes has lower variance than Logistic Regression. This is a drawback of Naive Bayes as predictors are not always independent of each other. However, this is a strength of Logistic Regression as it can find relationships between predictors. Although, a drawback of Logistic Regression is that it tends to overfit. This usually occurs when there are too many predictors and the model tries to satisfy each relationship, rather than trying to find the underlying trends. 
When it comes to choosing one method over the other, it is best to use both and see how the values differ.


## Strengths and Weaknesses of Classification Metrics
The accuracy is an easy and necessary metric used to determine if the model is a good fit. Kappa is used to ensure that correctness by chance is factored in. Sensitivity and Specificity are used to determine correct positive and negative values respectively. The ROC measures how the specificity and sensitivity are related to each other, and the AUC is the area under the curve. The major drawback about using these metrics is that skewed data will result in incorrect values.