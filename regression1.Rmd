---
title: "Linear Regression"
author: "Sovanna Ramirez & Sanjana Jadhav"
date: "2023-02-17"
output:
  pdf_document: default
  html_notebook: default
  html_document:
    df_print: paged
---
In this notebook, linear regression will be explored through various practices on a given data set. Before getting into data exploration let’s first understand what linear regression is and how it works. Linear regression is a machine learning algorithm most commonly used for statistics. With linear regression we are trying to determine if there is a linear relationship between predictor (x) and target (y). Some of the benefits of linear regression is that it is both simple and powerful, it works well when data follows a linear pattern, and it has low variance. The downside to linear regression is that it has high bias due to the fact that the model inherently follows a linear shape to the data.

## Data Exploration
This example looks at the data set **Diamonds** as an intro into linear regression. The data set was downloaded from here: https://www.kaggle.com/datasets/shivam2503/diamonds

To start, the data set is read into and stored in a data frame.
```{r}
df <- read.csv("~/Downloads/diamonds.csv")
```
**Data Cleaning**: Checking for N/A values
```{r}
sapply(df,function(x) sum(is.na(x)==TRUE))
```
**Divide into 80/20 train/test**: From here we divide the data set. Where 80% of the original data is dedicated to the training set and 20% of the original data is dedicated to the testing set. Once the data is divided, we can use the training set to practice some data exploration…
```{r}
i <- sample(1:nrow(df), nrow(df)*0.80, replace=FALSE)
train <- df[i,]
test <- df[-i,]
```
### The summary() function
With the **summary()** function we are provided with the summary statistics of the data from the training set. Our variables for this data set include carat (weight of the diamond), cut, color, clarity, depth, table, price, length in mm (x), width in mm (y), and depth in mm (z).
```{r}
summary(train)
```
### The head() function 
The **head()** function will output the first n rows from the training set. In our case, the head() function will print out the first 20 rows.
```{r}
head(train, n=20)
```
### The tail() function
Similar to the head() function, the **tail()** function will output the last n rows from the training set. Here the tail() function will print out the last 20 rows of our training set.
```{r}
tail(train, n=20)
```
### The str() function
The **str()** function will output the structure of the data frame. The str() function provides us with the number of observations or instances in our training set. As well as the data type for each of the variables.
```{r}
str(train)
```
### The dim() function
The **dim()** function will output the dimensions of the data frame in terms of rows and columns. There are 43152 rows and 11 variables in our training set.
```{r}
dim(train)
```



## Data Visualization
Now that we have done some data exploration and sought to understand a few functions in r, we can go into data visualization. Data visualization allows us to understand data through visual representations. To start, let's explore some simple graphs using graphing functions in R.


For the first graph, the **hist()** function will be used to create a histogram with carat as its parameter. Carat is the weight of a diamond ranging from 0.2-5.01.
```{r}
hist(train$carat, main="Carat (weight of diamond)",
xlab="Carat")
```


For our second graph, the **plot()** function will be used to create a scatter plot with parameters carat and price. In this plot we start to see a potential correlation between the price of a diamond and the carat or weight of a diamond. 
```{r}
plot(train$carat, train$price, main="Carat v. Price", xlab = "Carat", ylab = "Price")
```

### Simple Linear Regression Model
Now using the training data, we build a simple linear regression model which demonstrates how *carat* (weight of a diamond) has an affect on the *price* of a diamond. Our summary of lm1 provides the statistics to better understand the relationship between the predictor and our target.


* **Residual standard error** is meant to be a smaller number. For this demonstration, our RSE was 1547. A large RSE means that our data points are far from the fitted regression line.
* Typically the **r-squared value** should be closer to 1 to represent the "goodness of fit." In our case the data does fit the linear regression model decently with an r-squared value of 0.8507
* The **f-statistic and p-value** can be used together to determine how confident the model is. With our f-statistic at 2.458e+05 and a low p-value we may conclude that there is confidence in the model.

```{r}
lm1 <-lm(price~carat, data=train)
summary(lm1)
```
Now to plot the residuals...

* The **Residuals vs Fitted** plot demonstrates no particular pattern with residuals which are not equally spread out. Thus, this plot indicates that we may not have a non-linear relationship between our predictor (carat) and our outcome (price).
* The **Normal Q-Q** plot shows that the residuals deviate from the line pretty severely. Thus, indicating that the residuals are not normally distributed.
* The **Scale-Location** plot shows that the line is not horizontal and the points are not equally spread out. We can conclude that the residuals are not spread equally among the range of predictors.
* The **Residuals vs Leverage** plot shows that there are cases which may be influential to the regression results. These cases are found at the lower right corner and if excluded will change our regression results.
```{r}
par(mfrow=c(2,2))
plot(lm1)
```

### Multiple Linear Regression Model
Using the training data, we will explore a multiple linear regression model with multiple predictors. First, let's explore how depth and carat affect the price of a diamond.
```{r}
lm2 <- lm(price~depth + carat, data=train)
summary(lm2)
```
Now to plot the residuals of the multiple linear regression model lm2...


After plotting the residuals on the training set it is still quite tricky to tell if the models fit the data well or not. However, we do see a slight improvement from lm1 (a simple linear regression model) to lm2 (a multiple linear regression model) using the RSE and r-squared values.
```{r}
par(mfrow=c(2,2))
plot(lm2)
```


Now let's explore a different combination of predictors that may improve our results for our multiple linear regression model. 


In this next model lm3 we will see how length in mm (x) and carat affect the price of a diamond.
```{r}
lm3 <- lm(price~x + carat, data=train)
summary(lm3)
```
The residual plots for lm3 seem to show some improvement. Again, looking at the summary statistics we can see that lm3 is an improvement upon lm2.
```{r}
par(mfrow=c(2,2))
plot(lm3)
```


### Summary of results
When comparing all three linear models lm1, lm2, and lm3 lm3 is the best model out of the three. To recap, lm1 was our simple linear regression model in which we wanted to examine if there was a linear relationship between the carat, weight of a diamond, and the price of a diamond. Moving to lm2, lm2 examines how the combination of the depth and carat influence the price of a diamond. And finally lm3, examines how the combination of the length in mm (x) and the carat influence the price of a diamond. Although the residual plots were quite tricky to understand due to the mass amount of data we were performing on, the summary statistics provided us a clearer understanding of which was the best model. And to conclude, lm3's summary provided us the statistics needed to determine it was the best model. It had the lowest residual standard error of 1524, an r-squared closest to 1 with 0.855, and high f-statistic with low p-value. Thus, lm3 was the best model out of the three.


### Predict and evaluate on test data
Using the three models lm1, lm2, and lm3 we can now evaluate our test data set using metrics correlation and MSE. These metrics will help us evaluate our prediction accuracy. Moreover, correlation helps us to understand how strong a relationship between variables is. With a correlation close to +1 would indicate a positive correlation between variables. While mean square error (MSE) helps us understand the amount of error in our regression models. With MSE we take the difference between the actual values and the predicted values. An MSE of 0 indicates no errors. Now, since our models were not the best representations of a good fit, we can expect to get poor results when calculating correlation and mse.


After performing metrics correlation and MSE we see that lm3 has the highest correlation closest to +1 with 0.92 and the lowest MSE. Although lm3's MSE is lowest in comparison to the other models, this model may not be able to make accurate predictions of the target variable. For future improvements in multiple linear regression models, we may want to consider including more than two predictors.

**lm1:**
```{r}
pred1 <- predict(lm1, newdata=test)
```

```{r}
pred1 <- predict(lm1, newdata=test)
cor1 <- cor(pred1, test$price)
mse1 <- mean((pred1-test$price)^2) 
rmse1 <- sqrt(mse1)

print(paste('correlation:', cor1))
print(paste('mse:', mse1))
print(paste('rmse:', rmse1))
```


**lm2:**
```{r}
pred2 <- predict(lm2, newdata=test)
cor2 <- cor(pred2, test$price)
mse2 <- mean((pred2-test$price)^2) 
rmse2 <- sqrt(mse2)

print(paste('correlation:', cor2))
print(paste('mse:', mse2))
print(paste('rmse:', rmse2))
```


**lm3:**
```{r}
pred3 <- predict(lm3, newdata=test)
cor3 <- cor(pred3, test$price)
mse3 <- mean((pred3-test$price)^2) 
rmse3 <- sqrt(mse3)

print(paste('correlation:', cor3))
print(paste('mse:', mse3))
print(paste('rmse:', rmse3))
```

